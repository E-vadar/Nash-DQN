{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default simulation parameters\n",
    "sim_dict = {'perm_price_impact': .3,\n",
    "            'transaction_cost': .5,\n",
    "            'liquidation_cost': .5,\n",
    "            'running_penalty': 0,\n",
    "            'T': 4,\n",
    "            'dt': 1,\n",
    "            'N_agents': 5,\n",
    "            'drift_function': (lambda x, y: 0.1*(10-y)),\n",
    "            'volatility': 1,\n",
    "            'initial_price_var': 20}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = {'perm_price_impact':0.01,\n",
    "            'trans_impact_scale':0.0,\n",
    "            'trans_impact_decay':0.0,\n",
    "            'transaction_cost':0.01,\n",
    "            'liquidation_cost':0.05,\n",
    "            'running_penalty':0.02,\n",
    "            'T':5,\n",
    "            'dt':1/12,\n",
    "            'N_agents':5,\n",
    "            'drift_function': lambda x,y: 0,\n",
    "            'volatility':0.01,\n",
    "            'initial_price_var':0.1,\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/brian/Nash-DQN/Nash DQN - Final/NashAgent_lib.py:274: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  uNeg_list = torch.tensor(self.matrix_slice(action_list)).float().cpu()\n",
      "/home/brian/Nash-DQN/Nash DQN - Final/NashAgent_lib.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  act_list = torch.tensor(action_list.view(-1)).float().cpu()\n",
      "/home/brian/Nash-DQN/Nash DQN - Final/NashRL.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.clamp(torch.tensor(a).detach(), -max_a, max_a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Simulation: 0 \n",
      " State(t=5.0, p=9.998954, i=-0.002647464954116631, q=array([ 0.0393,  0.0446,  0.0055, -0.0313, -0.2679]))\n",
      "Current State: \n",
      "State(t=5.0, p=9.998954, i=-0.002647464954116631, q=array([ 0.0393,  0.0446,  0.0055, -0.0313, -0.2679]))\n",
      "Action taken: {} tensor([ 4.7400,  1.1128,  3.9073, -1.4490,  8.7432], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=4.0, p=10.030531126252203, i=-0.002647464954116631, q=array([ 4.7793,  1.1574,  3.9129, -1.4802,  8.4753]))\n",
      "Nash Action: [-0.1389 -0.1388 -0.1395 -0.1401 -0.1443], Nash Value: [[-1.2595 -1.2594 -1.2606 -1.2617 -1.2687]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=10.030531126252203, i=-0.002647464954116631, q=array([ 4.7793,  1.1574,  3.9129, -1.4802,  8.4753]))\n",
      "Action taken: {} tensor([-0.1151, -0.0495, -0.0909, -0.0919, -0.2184])\n",
      "Ending State: \n",
      "State(t=3.0, p=10.01757829639304, i=-0.002647464954116631, q=array([ 4.6642,  1.1079,  3.822 , -1.5721,  8.2569]))\n",
      "Nash Action: [0.8678 0.785  0.8528 0.729  0.9039], Nash Value: [[-2.9885 -4.2124 -3.2809 -4.8885 -1.7094]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=10.01757829639304, i=-0.002647464954116631, q=array([ 4.6642,  1.1079,  3.822 , -1.5721,  8.2569]))\n",
      "Action taken: {} tensor([-1.0470, -6.9903, -6.7053,  5.2232, -4.0058], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=2.0, p=9.97825466573569, i=-0.002647464954116631, q=array([ 3.6171, -5.8824, -2.8833,  3.6511,  4.2512]))\n",
      "Nash Action: [0.3055 0.2442 0.2911 0.2131 0.3671], Nash Value: [[-2.1092 -4.903  -2.7764 -7.2129 -0.3746]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=9.97825466573569, i=-0.002647464954116631, q=array([ 3.6171, -5.8824, -2.8833,  3.6511,  4.2512]))\n",
      "Action taken: {} tensor([0.2599, 0.2155, 0.2121, 0.2605, 0.2684])\n",
      "Ending State: \n",
      "State(t=1.0, p=9.985841157258063, i=-0.002647464954116631, q=array([ 3.8771, -5.6669, -2.6712,  3.9116,  4.5196]))\n",
      "Nash Action: [0.3209 0.233  0.2579 0.3212 0.3269], Nash Value: [[ -2.8205 -17.7201 -12.3219  -2.7795  -2.0552]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=9.985841157258063, i=-0.002647464954116631, q=array([ 3.8771, -5.6669, -2.6712,  3.9116,  4.5196]))\n",
      "Action taken: {} tensor([0.3245, 0.2011, 0.2393, 0.3249, 0.3306])\n",
      "Ending State: \n",
      "State(t=0.0, p=9.9917030003837, i=-0.002647464954116631, q=array([ 4.2016, -5.4658, -2.432 ,  4.2365,  4.8502]))\n",
      "Nash Action: [0.3972 0.1546 0.2114 0.3981 0.4126], Nash Value: [[  0.0434 -16.1698  -5.2817   0.0445   0.0645]]\n",
      "\n",
      "Current Loss: 27991.289485931396\n",
      "\n",
      "\n",
      "New Simulation: 30 \n",
      " State(t=5.0, p=10.00286, i=-0.003485292888385013, q=array([-10.7614,  22.8992,   1.1903,   4.8962,   0.3076]))\n",
      "Current State: \n",
      "State(t=5.0, p=10.00286, i=-0.003485292888385013, q=array([-10.7614,  22.8992,   1.1903,   4.8962,   0.3076]))\n",
      "Action taken: {} tensor([-0.1238, -1.4156, -0.7399, -1.0848, -0.6382])\n",
      "Ending State: \n",
      "State(t=4.0, p=9.981639227241061, i=-0.003485292888385013, q=array([-10.8852,  21.4836,   0.4504,   3.8114,  -0.3306]))\n",
      "Nash Action: [-0.1514 -1.4639 -0.7707 -1.1567 -0.6832], Nash Value: [[-127.6424  177.911    -1.661    34.2234  -10.1883]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=9.981639227241061, i=-0.003485292888385013, q=array([-10.8852,  21.4836,   0.4504,   3.8114,  -0.3306]))\n",
      "Action taken: {} tensor([ 21.5970, -12.3139,  17.2075,  -5.1177,   9.5065], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=3.0, p=10.041055620374793, i=-0.003485292888385013, q=array([10.7118,  9.1697, 17.6579, -1.3063,  9.176 ]))\n",
      "Nash Action: [-0.1531 -1.4366 -0.697  -1.0333 -0.6106], Nash Value: [[-127.1791  167.9477   -6.2901   26.2358  -13.8684]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=10.041055620374793, i=-0.003485292888385013, q=array([10.7118,  9.1697, 17.6579, -1.3063,  9.176 ]))\n",
      "Action taken: {} tensor([ -8.0511, -18.2571, -25.1999,  10.0313, -15.9808], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=2.0, p=9.90941788782116, i=-0.003485292888385013, q=array([ 2.6607, -9.0874, -7.542 ,  8.7251, -6.8049]))\n",
      "Nash Action: [-1.84   -1.7642 -1.9239 -1.0698 -1.7646], Nash Value: [[ 83.1037  67.7906 142.8205 -35.6307  67.8531]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=9.90941788782116, i=-0.003485292888385013, q=array([ 2.6607, -9.0874, -7.542 ,  8.7251, -6.8049]))\n",
      "Action taken: {} tensor([ -4.6884,   9.5736,   9.3784, -24.1347,  15.6903], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=1.0, p=9.919049626867157, i=-0.003485292888385013, q=array([ -2.0277,   0.4863,   1.8364, -15.4097,   8.8854]))\n",
      "Nash Action: [-1.1963 -0.2226 -0.341  -1.8604 -0.3978], Nash Value: [[  19.4709 -100.1561  -82.5512   76.9615  -73.9902]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=9.919049626867157, i=-0.003485292888385013, q=array([ -2.0277,   0.4863,   1.8364, -15.4097,   8.8854]))\n",
      "Action taken: {} tensor([ -1.6311,  -8.5422,  20.6911,  17.5143, -20.1044], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=0.0, p=9.939784050009916, i=-0.003485292888385013, q=array([ -3.6588,  -8.0559,  22.5276,   2.1046, -11.219 ]))\n",
      "Nash Action: [-1.7085 -2.0725 -2.2872 -0.5057 -3.9349], Nash Value: [[ -35.0058  -10.3257    2.8222 -190.6623   71.5764]]\n",
      "\n",
      "Current Loss: 21250.85228729248\n",
      "\n",
      "\n",
      "New Simulation: 60 \n",
      " State(t=5.0, p=9.9953, i=-0.0015737169758904615, q=array([ 10.4729, -12.9297,  -6.5223,   2.9699,  11.1072]))\n",
      "Current State: \n",
      "State(t=5.0, p=9.9953, i=-0.0015737169758904615, q=array([ 10.4729, -12.9297,  -6.5223,   2.9699,  11.1072]))\n",
      "Action taken: {} tensor([-2.7895e+00, -2.2925e-03, -5.8502e-02, -9.5736e-01, -2.9440e+00])\n",
      "Ending State: \n",
      "State(t=4.0, p=9.975099499354814, i=-0.0015737169758904615, q=array([  7.6835, -12.932 ,  -6.5808,   2.0125,   8.1632]))\n",
      "Nash Action: [-2.7996e+00  2.5135e-03 -5.5177e-02 -9.5294e-01 -2.9570e+00], Nash Value: [[  89.7174 -154.1176  -79.9661   16.0623   95.944 ]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=9.975099499354814, i=-0.0015737169758904615, q=array([  7.6835, -12.932 ,  -6.5808,   2.0125,   8.1632]))\n",
      "Action taken: {} tensor([ -9.3473,  11.6298,  13.0089,  -4.0918, -20.3594], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=3.0, p=9.977259615182847, i=-0.0015737169758904615, q=array([ -1.6638,  -1.3022,   6.4281,  -2.0793, -12.1962]))\n",
      "Nash Action: [-2.4075 -0.0274 -0.0807 -1.034  -2.5277], Nash Value: [[  54.6725 -164.3829  -88.5438   -1.2295   59.4016]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=9.977259615182847, i=-0.0015737169758904615, q=array([ -1.6638,  -1.3022,   6.4281,  -2.0793, -12.1962]))\n",
      "Action taken: {} tensor([ 8.3771,  1.2729, -9.1405,  8.3811,  6.0983], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=2.0, p=10.01389366531774, i=-0.0015737169758904615, q=array([ 6.7132, -0.0294, -2.7124,  6.3018, -6.0979]))\n",
      "Nash Action: [-0.5145 -0.5811 -2.1521 -0.4436 -0.03  ], Nash Value: [[ -18.521   -14.8485   59.409   -22.7456 -141.4591]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=10.01389366531774, i=-0.0015737169758904615, q=array([ 6.7132, -0.0294, -2.7124,  6.3018, -6.0979]))\n",
      "Action taken: {} tensor([-9.0320,  2.5185, -0.5656,  6.3670,  9.0598], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=1.0, p=10.028399472051841, i=-0.0015737169758904615, q=array([-2.3188,  2.4892, -3.278 , 12.6687,  2.9619]))\n",
      "Nash Action: [-3.01   -1.0661 -0.5477 -2.9127 -0.057 ], Nash Value: [[ 46.3474 -23.133  -51.93    42.1269 -90.602 ]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=10.028399472051841, i=-0.0015737169758904615, q=array([-2.3188,  2.4892, -3.278 , 12.6687,  2.9619]))\n",
      "Action taken: {} tensor([-3.0149, -9.1510,  8.6530, -6.3458,  4.3013], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=0.0, p=10.028861211780374, i=-0.0015737169758904615, q=array([-5.3337, -6.6618,  5.3749,  6.3229,  7.2632]))\n",
      "Nash Action: [-2.3629 -3.7481 -2.112  -9.966  -3.8885], Nash Value: [[-24.3861  21.8274 -34.0338 108.9309  26.2689]]\n",
      "\n",
      "Current Loss: 1146.6381912231445\n",
      "\n",
      "\n",
      "New Simulation: 90 \n",
      " State(t=5.0, p=10.010668, i=-0.0010851769601568325, q=array([ -3.3905,  -6.3394,  12.6181, -14.9175,  -5.1559]))\n",
      "Current State: \n",
      "State(t=5.0, p=10.010668, i=-0.0010851769601568325, q=array([ -3.3905,  -6.3394,  12.6181, -14.9175,  -5.1559]))\n",
      "Action taken: {} tensor([ 0.0964,  0.3378, -4.2622,  0.5827,  0.2455])\n",
      "Ending State: \n",
      "State(t=4.0, p=9.996137949844684, i=-0.0010851769601568325, q=array([ -3.2941,  -6.0015,   8.3559, -14.3349,  -4.9103]))\n",
      "Nash Action: [ 0.2697  0.48   -3.721   0.6059  0.409 ], Nash Value: [[ -51.3302  -84.5512  108.6364 -186.3838  -70.5751]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=9.996137949844684, i=-0.0010851769601568325, q=array([ -3.2941,  -6.0015,   8.3559, -14.3349,  -4.9103]))\n",
      "Action taken: {} tensor([-2.1964, -2.7387, -9.3575,  8.0701,  6.3771], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=3.0, p=9.990407446785154, i=-0.0010851769601568325, q=array([-5.4905, -8.7402, -1.0016, -6.2648,  1.4668]))\n",
      "Nash Action: [ 0.0713  0.4073 -2.4116  0.601   0.2718], Nash Value: [[ -50.2562  -79.6564   63.2639 -172.3004  -67.2198]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=9.990407446785154, i=-0.0010851769601568325, q=array([-5.4905, -8.7402, -1.0016, -6.2648,  1.4668]))\n",
      "Action taken: {} tensor([ 2.6381,  2.9225,  5.8785,  4.0135, -5.9562], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=2.0, p=10.002787685027199, i=-0.0010851769601568325, q=array([-2.8524, -5.8177,  4.8769, -2.2513, -4.4894]))\n",
      "Nash Action: [ 0.1677  0.5089 -0.4166  0.2627 -1.0137], Nash Value: [[ -66.1595 -102.4244  -16.9858  -75.2894    7.3555]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: \n",
      "State(t=2.0, p=10.002787685027199, i=-0.0010851769601568325, q=array([-2.8524, -5.8177,  4.8769, -2.2513, -4.4894]))\n",
      "Action taken: {} tensor([-0.2600,  0.1276, -2.3758, -0.3500, -0.0465])\n",
      "Ending State: \n",
      "State(t=1.0, p=10.015392556180066, i=-0.0010851769601568325, q=array([-3.1124, -5.69  ,  2.5011, -2.6012, -4.5359]))\n",
      "Nash Action: [-0.1107  0.2704 -2.1107 -0.2037  0.097 ], Nash Value: [[-47.1199 -80.2526  31.445  -40.1655 -66.0619]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=10.015392556180066, i=-0.0010851769601568325, q=array([-3.1124, -5.69  ,  2.5011, -2.6012, -4.5359]))\n",
      "Action taken: {} tensor([-1.6520, -1.2003, -3.1159, -1.7835, -1.4052])\n",
      "Ending State: \n",
      "State(t=0.0, p=9.998892762255306, i=-0.0010851769601568325, q=array([-4.7644, -6.8904, -0.6148, -4.3847, -5.9412]))\n",
      "Nash Action: [-1.8042 -1.3423 -3.3449 -1.9174 -1.5528], Nash Value: [[-30.6675 -58.2373  24.2226 -25.3974 -46.1881]]\n",
      "\n",
      "Current Loss: 1374.267912864685\n",
      "\n",
      "\n",
      "New Simulation: 120 \n",
      " State(t=5.0, p=9.998589, i=-0.006121597046885977, q=array([-0.5027,  8.7293,  1.2985,  2.2489, 14.0041]))\n",
      "Current State: \n",
      "State(t=5.0, p=9.998589, i=-0.006121597046885977, q=array([-0.5027,  8.7293,  1.2985,  2.2489, 14.0041]))\n",
      "Action taken: {} tensor([ 1.0073, -2.7491,  0.3310, -0.0340, -7.1677])\n",
      "Ending State: \n",
      "State(t=4.0, p=9.999410334098673, i=-0.006121597046885977, q=array([0.5047, 5.9802, 1.6295, 2.2149, 6.8364]))\n",
      "Nash Action: [ 0.9353 -2.793   0.2566 -0.1035 -7.1193], Nash Value: [[-21.1745  70.9907  -3.2611   6.2228 121.7706]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=9.999410334098673, i=-0.006121597046885977, q=array([0.5047, 5.9802, 1.6295, 2.2149, 6.8364]))\n",
      "Action taken: {} tensor([ 0.4477, -1.9502, -0.0446, -0.3003, -2.3249])\n",
      "Ending State: \n",
      "State(t=3.0, p=9.982504726222555, i=-0.006121597046885977, q=array([0.9524, 4.03  , 1.585 , 1.9146, 4.5115]))\n",
      "Nash Action: [ 0.4541 -1.9415 -0.0376 -0.293  -2.3158], Nash Value: [[-7.3291 46.039   3.6287  9.3326 54.3875]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=9.982504726222555, i=-0.006121597046885977, q=array([0.9524, 4.03  , 1.585 , 1.9146, 4.5115]))\n",
      "Action taken: {} tensor([  5.1804,   8.5896, -10.4659,   7.6772,   0.5634], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=2.0, p=10.006005141448702, i=-0.006121597046885977, q=array([ 6.1328, 12.6196, -8.881 ,  9.5918,  5.0749]))\n",
      "Nash Action: [ 0.0474 -1.3166 -0.2309 -0.3769 -1.5306], Nash Value: [[-3.882  27.0654  2.4691  5.7813 31.9176]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=10.006005141448702, i=-0.006121597046885977, q=array([ 6.1328, 12.6196, -8.881 ,  9.5918,  5.0749]))\n",
      "Action taken: {} tensor([-2.2953, -8.4501,  2.5230, -5.4549, -1.8251])\n",
      "Ending State: \n",
      "State(t=1.0, p=9.979606872201252, i=-0.006121597046885977, q=array([ 3.8375,  4.1696, -6.3579,  4.1369,  3.2498]))\n",
      "Nash Action: [-2.3633 -8.6561  2.4967 -5.6798 -1.894 ], Nash Value: [[  50.9006  114.951  -106.7548   85.0542   40.4552]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=9.979606872201252, i=-0.006121597046885977, q=array([ 3.8375,  4.1696, -6.3579,  4.1369,  3.2498]))\n",
      "Action taken: {} tensor([ 1.5885, -3.0275,  2.4381, -4.6687, -0.2356], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=0.0, p=9.972911052704356, i=-0.006121597046885977, q=array([ 5.426 ,  1.1421, -3.9198, -0.5318,  3.0141]))\n",
      "Nash Action: [-4.2488 -4.7057  0.4655 -4.6724 -3.2423], Nash Value: [[ 27.4609  30.7125 -81.3379  30.3928  21.7111]]\n",
      "\n",
      "Current Loss: 761.75075340271\n",
      "\n",
      "\n",
      "New Simulation: 150 \n",
      " State(t=5.0, p=10.015533, i=-0.00160067692330579, q=array([-11.9107,   1.5138,   8.109 ,   0.454 ,   0.8039]))\n",
      "Current State: \n",
      "State(t=5.0, p=10.015533, i=-0.00160067692330579, q=array([-11.9107,   1.5138,   8.109 ,   0.454 ,   0.8039]))\n",
      "Action taken: {} tensor([10.7163,  0.4037, -3.9020,  1.3156,  1.0130])\n",
      "Ending State: \n",
      "State(t=4.0, p=10.013748785364037, i=-0.00160067692330579, q=array([-1.1944,  1.9175,  4.207 ,  1.7696,  1.8169]))\n",
      "Nash Action: [10.4224 -0.0985 -4.6191  0.8084  0.5071], Nash Value: [[-133.899     1.0154   66.231    -9.4564   -5.9988]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=10.013748785364037, i=-0.00160067692330579, q=array([-1.1944,  1.9175,  4.207 ,  1.7696,  1.8169]))\n",
      "Action taken: {} tensor([ 2.5184, -0.2573, -1.5372, -0.1279, -0.1693])\n",
      "Ending State: \n",
      "State(t=3.0, p=10.012951861654582, i=-0.00160067692330579, q=array([1.324 , 1.6601, 2.6698, 1.6417, 1.6476]))\n",
      "Nash Action: [ 2.2405 -0.5656 -1.7492 -0.4346 -0.4765], Nash Value: [[-22.5153   6.8213  28.7069   5.4089   5.8602]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=10.012951861654582, i=-0.00160067692330579, q=array([1.324 , 1.6601, 2.6698, 1.6417, 1.6476]))\n",
      "Action taken: {} tensor([-3.5805,  0.3101, -7.5217,  1.5893, -4.8208], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=2.0, p=9.984885913956429, i=-0.00160067692330579, q=array([-2.2566,  1.9702, -4.8519,  3.231 , -3.1732]))\n",
      "Nash Action: [ 0.7432  0.4318 -0.498   0.4488  0.4434], Nash Value: [[ 3.1256  6.4794 16.5551  6.296   6.3546]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=9.984885913956429, i=-0.00160067692330579, q=array([-2.2566,  1.9702, -4.8519,  3.231 , -3.1732]))\n",
      "Action taken: {} tensor([ 3.8428,  0.3823,  6.2267, -0.2209,  4.6823])\n",
      "Ending State: \n",
      "State(t=1.0, p=10.019830005673382, i=-0.00160067692330579, q=array([1.5862, 2.3526, 1.3748, 3.0102, 1.5091]))\n",
      "Nash Action: [ 2.7516 -0.2705  5.0889 -0.8679  3.5737], Nash Value: [[-31.3621   9.9475 -57.9024  22.0798 -41.0373]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=10.019830005673382, i=-0.00160067692330579, q=array([1.5862, 2.3526, 1.3748, 3.0102, 1.5091]))\n",
      "Action taken: {} tensor([-5.5543, -8.3499, -7.5701, -0.4425, -0.9646], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=0.0, p=9.982966849147674, i=-0.00160067692330579, q=array([-3.9681, -5.9973, -6.1953,  2.5677,  0.5445]))\n",
      "Nash Action: [-0.7636 -1.1279 -0.5751 -1.8328 -0.6971], Nash Value: [[ 8.82   16.3717  6.7369 22.8523  8.0604]]\n",
      "\n",
      "Current Loss: 200.3791627883911\n",
      "\n",
      "\n",
      "New Simulation: 180 \n",
      " State(t=5.0, p=9.992787, i=-0.003909925655292951, q=array([ 8.4549, 14.4707, -7.3206, -2.7439, -1.2806]))\n",
      "Current State: \n",
      "State(t=5.0, p=9.992787, i=-0.003909925655292951, q=array([ 8.4549, 14.4707, -7.3206, -2.7439, -1.2806]))\n",
      "Action taken: {} tensor([-4.9661, -9.9654,  5.2106,  1.0393, -0.2577])\n",
      "Ending State: \n",
      "State(t=4.0, p=9.973696252679485, i=-0.003909925655292951, q=array([ 3.4889,  4.5052, -2.11  , -1.7046, -1.5382]))\n",
      "Nash Action: [ -5.1443 -10.1985   5.5569   1.4409   0.1572], Nash Value: [[ 78.1979 138.7704 -85.9261 -35.2718 -20.1325]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=9.973696252679485, i=-0.003909925655292951, q=array([ 3.4889,  4.5052, -2.11  , -1.7046, -1.5382]))\n",
      "Action taken: {} tensor([-12.7428, -12.4846,  -7.5230,  -1.5087,  -2.9634], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=3.0, p=9.908839191078147, i=-0.003909925655292951, q=array([-9.254 , -7.9794, -9.6331, -3.2133, -4.5017]))\n",
      "Nash Action: [-1.5894 -2.0543  1.8714  1.4983  1.3454], Nash Value: [[ 17.4643  27.1548 -35.9321 -32.1627 -30.5804]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=9.908839191078147, i=-0.003909925655292951, q=array([-9.254 , -7.9794, -9.6331, -3.2133, -4.5017]))\n",
      "Action taken: {} tensor([9.5088, 8.3031, 9.8674, 3.7757, 5.0412])\n",
      "Ending State: \n",
      "State(t=2.0, p=9.985797300024746, i=-0.003909925655292951, q=array([0.2548, 0.3237, 0.2343, 0.5624, 0.5396]))\n",
      "Nash Action: [10.1888  8.988  10.546   4.4913  5.7451], Nash Value: [[-108.0995  -89.8445 -112.9892  -36.2079  -50.0263]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=9.985797300024746, i=-0.003909925655292951, q=array([0.2548, 0.3237, 0.2343, 0.5624, 0.5396]))\n",
      "Action taken: {} tensor([11.8523, -2.0268, -1.0071,  6.3497,  8.7127], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=1.0, p=10.028448255429169, i=-0.003909925655292951, q=array([12.1072, -1.7031, -0.7728,  6.9121,  9.2522]))\n",
      "Nash Action: [-0.835  -0.865  -0.826  -0.9691 -0.9592], Nash Value: [[-6.9451 -6.3238 -7.126  -4.0257 -4.2459]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=10.028448255429169, i=-0.003909925655292951, q=array([12.1072, -1.7031, -0.7728,  6.9121,  9.2522]))\n",
      "Action taken: {} tensor([-11.8028,   0.1481,  -0.7664,  -7.6146,  -9.5012])\n",
      "Ending State: \n",
      "State(t=0.0, p=9.96728999648083, i=-0.003909925655292951, q=array([ 0.3043, -1.555 , -1.5392, -0.7026, -0.249 ]))\n",
      "Nash Action: [-11.6324   0.5388  -0.3763  -7.4462  -9.3319], Nash Value: [[116.1196 -21.0826 -11.6042  66.026   89.616 ]]\n",
      "\n",
      "Current Loss: 507.97390365600586\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Simulation: 210 \n",
      " State(t=5.0, p=10.004403, i=-0.0012811412949231221, q=array([13.1962,  1.3461, -4.5799,  7.8435, -8.5994]))\n",
      "Current State: \n",
      "State(t=5.0, p=10.004403, i=-0.0012811412949231221, q=array([13.1962,  1.3461, -4.5799,  7.8435, -8.5994]))\n",
      "Action taken: {} tensor([-8.4249, -0.6862,  3.5498, -3.7608,  7.1309])\n",
      "Ending State: \n",
      "State(t=4.0, p=9.99647998015116, i=-0.0012811412949231221, q=array([ 4.7713,  0.6599, -1.0301,  4.0827, -1.4684]))\n",
      "Nash Action: [-8.837  -1.0373  2.8313 -4.139   6.3303], Nash Value: [[117.0807   2.7198 -56.6516  65.4306 -93.6692]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=9.99647998015116, i=-0.0012811412949231221, q=array([ 4.7713,  0.6599, -1.0301,  4.0827, -1.4684]))\n",
      "Action taken: {} tensor([-2.1123, -0.4638,  0.7652, -1.8624,  1.2100])\n",
      "Ending State: \n",
      "State(t=3.0, p=9.991543808918076, i=-0.0012811412949231221, q=array([ 2.659 ,  0.1961, -0.2649,  2.2203, -0.2585]))\n",
      "Nash Action: [-1.8896 -0.3372  0.8752 -1.8047  1.3111], Nash Value: [[ 34.6597  -5.5425 -22.3212  27.925  -26.9665]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=9.991543808918076, i=-0.0012811412949231221, q=array([ 2.659 ,  0.1961, -0.2649,  2.2203, -0.2585]))\n",
      "Action taken: {} tensor([-0.9576, -0.1178, -2.3172, 12.8226,  4.6551], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=2.0, p=10.03283654719293, i=-0.0012811412949231221, q=array([ 1.7013,  0.0784, -2.5821, 15.0428,  4.3966]))\n",
      "Nash Action: [-1.3094 -0.1752  0.2313 -1.1012  0.2248], Nash Value: [[ 14.6908  -9.2782 -13.7539  10.4209 -13.6914]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=10.03283654719293, i=-0.0012811412949231221, q=array([ 1.7013,  0.0784, -2.5821, 15.0428,  4.3966]))\n",
      "Action taken: {} tensor([ -9.3164,   4.7688,  -6.7906, -24.0748,  -5.5344], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=1.0, p=9.945485787687351, i=-0.0012811412949231221, q=array([-7.6151,  4.8472, -9.3727, -9.032 , -1.1379]))\n",
      "Nash Action: [ -0.719    0.0616   2.711  -11.4505  -2.0844], Nash Value: [[ 12.1053  -3.7815 -30.956  136.6393  38.4659]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=9.945485787687351, i=-0.0012811412949231221, q=array([-7.6151,  4.8472, -9.3727, -9.032 , -1.1379]))\n",
      "Action taken: {} tensor([ 5.8342, -5.0102, 21.0707,  6.5306,  6.6010], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=0.0, p=10.00965866331546, i=-0.0012811412949231221, q=array([-1.7809, -0.163 , 11.698 , -2.5014,  5.4631]))\n",
      "Nash Action: [ 7.0792 -4.84    8.7922  8.4601  0.7796], Nash Value: [[ -94.2895   38.7126 -115.2094 -111.1538  -20.2045]]\n",
      "\n",
      "Current Loss: 1098.1607685089111\n",
      "\n",
      "\n",
      "New Simulation: 240 \n",
      " State(t=5.0, p=9.99978, i=0.0008019124380719574, q=array([16.3772, -4.3907, 10.3191,  0.3901, 15.6752]))\n",
      "Current State: \n",
      "State(t=5.0, p=9.99978, i=0.0008019124380719574, q=array([16.3772, -4.3907, 10.3191,  0.3901, 15.6752]))\n",
      "Action taken: {} tensor([-22.2963,   2.3960,  -8.3804,  -2.3509, -21.9547], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=4.0, p=9.902309507145773, i=0.0008019124380719574, q=array([-5.9191, -1.9947,  1.9387, -1.9608, -6.2795]))\n",
      "Nash Action: [-11.8385   2.4839  -6.6426  -0.6357 -11.4906], Nash Value: [[149.0581 -57.1967  89.4706  -8.1731 142.1528]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=9.902309507145773, i=0.0008019124380719574, q=array([-5.9191, -1.9947,  1.9387, -1.9608, -6.2795]))\n",
      "Action taken: {} tensor([ 5.2359,  1.7558, -0.7266,  1.7218,  5.5336])\n",
      "Ending State: \n",
      "State(t=3.0, p=9.944595096360105, i=0.0008019124380719574, q=array([-0.6832, -0.239 ,  1.2121, -0.239 , -0.7459]))\n",
      "Nash Action: [ 5.3641  1.8409 -0.5956  1.8071  5.6593], Nash Value: [[-72.447  -32.6857   7.3167 -32.3238 -75.5631]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=9.944595096360105, i=0.0008019124380719574, q=array([-0.6832, -0.239 ,  1.2121, -0.239 , -0.7459]))\n",
      "Action taken: {} tensor([ 0.5731,  0.3730, -0.2638,  0.3730,  0.6377])\n",
      "Ending State: \n",
      "State(t=2.0, p=9.925484707352766, i=0.0008019124380719574, q=array([-0.1101,  0.134 ,  0.9484,  0.134 , -0.1082]))\n",
      "Nash Action: [ 0.5237  0.3305 -0.3141  0.3305  0.5627], Nash Value: [[-13.5749  -9.4612   4.2159  -9.4613 -14.1839]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=9.925484707352766, i=0.0008019124380719574, q=array([-0.1101,  0.134 ,  0.9484,  0.134 , -0.1082]))\n",
      "Action taken: {} tensor([-1.3245,  4.6126,  3.9665, -5.8969, 11.2239], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=1.0, p=9.954008718022422, i=0.0008019124380719574, q=array([-1.4346,  4.7465,  4.9148, -5.7629, 11.1157]))\n",
      "Nash Action: [ 0.3121  0.2038 -0.1603  0.2038  0.3113], Nash Value: [[-10.0522  -7.7844   0.0338  -7.7845 -10.0338]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=9.954008718022422, i=0.0008019124380719574, q=array([-1.4346,  4.7465,  4.9148, -5.7629, 11.1157]))\n",
      "Action taken: {} tensor([  1.0531,  -4.8219,  -4.9747,   5.1685, -10.6056])\n",
      "Ending State: \n",
      "State(t=0.0, p=9.936567818650547, i=0.0008019124380719574, q=array([-0.3815, -0.0753, -0.0599, -0.5945,  0.5101]))\n",
      "Nash Action: [  0.8886  -4.8339  -4.9859   4.9793 -10.5851], Nash Value: [[-19.3031  43.1351  44.8288 -71.282  107.141 ]]\n",
      "\n",
      "Current Loss: 455.9721040725708\n",
      "\n",
      "\n",
      "New Simulation: 270 \n",
      " State(t=5.0, p=9.9971895, i=-0.004352749225663605, q=array([ 9.2506, -4.9577,  6.1704,  3.1015, -4.2153]))\n",
      "Current State: \n",
      "State(t=5.0, p=9.9971895, i=-0.004352749225663605, q=array([ 9.2506, -4.9577,  6.1704,  3.1015, -4.2153]))\n",
      "Action taken: {} tensor([-14.1630,   6.9469,  -7.7143,  -2.7125,  -1.5685], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=4.0, p=9.95454783185454, i=-0.004352749225663605, q=array([-4.9124,  1.9891, -1.5439,  0.389 , -5.7839]))\n",
      "Nash Action: [-6.1111  2.9503 -3.245  -0.5709  2.3792], Nash Value: [[ 82.4218 -58.1356  52.4482  22.5627 -50.3565]]\n",
      "\n",
      "Current State: \n",
      "State(t=4.0, p=9.95454783185454, i=-0.004352749225663605, q=array([-4.9124,  1.9891, -1.5439,  0.389 , -5.7839]))\n",
      "Action taken: {} tensor([ 2.9884, -4.2262, -0.2144, -3.9836, -1.0181], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=3.0, p=9.957712802392166, i=-0.004352749225663605, q=array([-1.9239, -2.237 , -1.7583, -3.5946, -6.802 ]))\n",
      "Nash Action: [ 4.8462 -0.1247  1.8244  0.5947  5.5396], Nash Value: [[-61.9523   7.6767 -27.2824  -8.0506 -69.6837]]\n",
      "\n",
      "Current State: \n",
      "State(t=3.0, p=9.957712802392166, i=-0.004352749225663605, q=array([-1.9239, -2.237 , -1.7583, -3.5946, -6.802 ]))\n",
      "Action taken: {} tensor([2.3858, 2.7001, 2.2194, 4.0640, 7.2148])\n",
      "Ending State: \n",
      "State(t=2.0, p=9.978320983019165, i=-0.004352749225663605, q=array([0.4619, 0.4631, 0.4611, 0.4694, 0.4128]))\n",
      "Nash Action: [1.8068 2.1097 1.6462 3.4241 6.5031], Nash Value: [[-26.7471 -30.152  -24.9455 -44.5876 -81.0901]]\n",
      "\n",
      "Current State: \n",
      "State(t=2.0, p=9.978320983019165, i=-0.004352749225663605, q=array([0.4619, 0.4631, 0.4611, 0.4694, 0.4128]))\n",
      "Action taken: {} tensor([0.2899, 0.2894, 0.2902, 0.2867, 0.3110])\n",
      "Ending State: \n",
      "State(t=1.0, p=9.991149275740176, i=-0.004352749225663605, q=array([0.7518, 0.7525, 0.7514, 0.7561, 0.7239]))\n",
      "Nash Action: [0.2246 0.224  0.2249 0.2211 0.2471], Nash Value: [[-1.7061 -1.6948 -1.7133 -1.6339 -2.1777]]\n",
      "\n",
      "Current State: \n",
      "State(t=1.0, p=9.991149275740176, i=-0.004352749225663605, q=array([0.7518, 0.7525, 0.7514, 0.7561, 0.7239]))\n",
      "Action taken: {} tensor([ -2.2598, -10.6566, -13.5450, -12.4679,   1.1850], dtype=torch.float64)\n",
      "Ending State: \n",
      "State(t=0.0, p=9.910886849555103, i=-0.004352749225663605, q=array([ -1.508 ,  -9.9041, -12.7937, -11.7119,   1.9088]))\n",
      "Nash Action: [-0.565  -0.5653 -0.5648 -0.5667 -0.5542], Nash Value: [[0.5795 0.5862 0.5754 0.6217 0.3045]]\n",
      "\n",
      "Current Loss: 698.0640316009521\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from simulation_lib import *\n",
    "from NashRL import *\n",
    "\n",
    "# Set global digit printing options\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Define Training and Model Parameters\n",
    "num_players = 5           # Total number of agents\n",
    "max_step = 5                    # Total number of time steps\n",
    "\n",
    "sim_dict['dt'] = sim_dict['T']/max_step\n",
    "\n",
    "\n",
    "sim_obj = MarketSimulator(sim_dict)\n",
    "net_non_inv_dim = len(sim_obj.get_state()[0].to_numpy())\n",
    "net_non_inv_dim -= sim_obj.N-1\n",
    "out_dim = 4\n",
    "\n",
    "nash_agent = NashNN(non_invar_dim=net_non_inv_dim,n_players=sim_obj.N,\n",
    "                    output_dim=4, max_steps=max_step, trans_cost=0.5, \n",
    "                    terminal_cost=0.5, num_moms=5)\n",
    "\n",
    "# current_state = sim_obj.get_state()[0]\n",
    "# expanded_states, inv_states = nash_agent.expand_list(\n",
    "#     [current_state], as_tensor=True)\n",
    "\n",
    "# invar_split = torch.split(inv_states, 1, dim=1)\n",
    "\n",
    "# nash_agent.action_net.moment_encoder_net(invar_split[0])\n",
    "\n",
    "# nash_agent.action_net.forward(\n",
    "#     invar_input=inv_states,\n",
    "#     non_invar_input=expanded_states)\n",
    "\n",
    "# run_Nash_Agent(sim_dict,num_sim=15000, AN_file_name=\"Action_Net\")\n",
    "\n",
    "nash_agent, loss_data = run_Nash_Agent(sim_dict, nash_agent=nash_agent, num_sim=1500, AN_file_name=\"Action_Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
